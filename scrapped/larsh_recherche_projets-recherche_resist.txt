Résist :Robustesse etÉthique desSystèmesIntelligents deSurveillance en présence d’Attaques par « Exemples Adverses »
Aux temps de crises sociétales (sanitaire, sécuritaire, etc.), le besoin de sécurité chez les individus et les sociétés tend à s’amplifier. Une des technologies émergentes qui peut contribuer à une agilité élevée des territoires face à des événements anormaux réside dans la vidéosurveillance intelligente. Cette technologie déploie des algorithmes d’apprentissage automatique (machine-learning) et d’apprentissage profond (deep-learning) afin d’automatiser des tâches complexes comme la détection de personnes, la reconnaissance d’action, le tracking d’individus, etc. Bien que ces systèmes aient démontré des capacités élevées à résoudre des problèmes d’envergure, leur démocratisation est toujours limitée.
Des systèmes d’IA et notamment d’apprentissage profond ont été récemment démontrés vulnérables à un type d’attaques appelé « Exemples Adverses ». Ces attaques peuvent être implémentés via un « patch » imprimé et porté par l’attaquant qui résulte en une altération totale du système intelligent cible, compromettant ainsi son intégrité et son efficacité. Pour les systèmes de surveillance automatique, les caméras intelligentes sont particulièrement vulnérables à ces attaques. Vu la criticité que ces systèmes peuvent avoir, assurer leur robustesse est un défi technologique majeur. Les défenses existantes demeurent insuffisantes, notamment dans un contexte réel, et aucune technique de robustesse ne considèrent la composante multi-vue.
Déployer les systèmes basés sur l’IA pour la sécurisation des espaces publics peut créer un dilemme éthique qui appelle des solutions juridiques. En effet, ces systèmes peuvent reconnaître des personnes, reconnaître des actions, faire du tracking ciblé, etc. L’enjeu est d’imaginer les moyens de conjuguer les progrès permis par cette technologie en matière de sécurité avec la garantie des libertés fondamentales, à commencer par le respect des données personnelles de chaque individu et le principe de la non-discrimination et le droit de tout citoyen de participer librement dans la vie démocratique qui se déroule sur la voie publique (liberté de la manifestation).
Orienté par le challenge d’aboutir à une IA responsable et de confiance, le projet RESIST se propose d’investiguer la robustesse et l’éthique des systèmes de vidéosurveillance basés sur l’IA en faisant travailler de concert des chercheurs en sciences dures et en sciences sociales, dans une perspective polytechnique.
L’introduction d’une nouvelle technologie n’est jamais neutre. Une investigation sera menée sur les risques de biais que les systèmes intelligents peuvent induire, ainsi que les risques de ces systèmes sur la vie privée des individus, sur leur droit de ne pas faire l’objet de discrimination et sur leur impact l’auto-détermination informationnelle des individus. Cette étude mêlant enjeux technologique, juridique et éthique débouchera sur des recommandations aussi bien méthodologiques que réglementaires et techniques pour développer une IA responsable. Plusieurs questions mériteront d’être soulevées en termes éthico-juridiques : quelles dérives sociétales ce type de surveillance biométrique est-il susceptible d’engendrer ? La vidéosurveillance intelligente ne comporte-t-elle pas notamment des risques juridiques majeurs ? Qu’est-ce qui garantit par exemple que les systèmes intelligents ne sont pas biaisés et générateurs de discriminations (à l’encontre des minorités visibles tout spécialement) ? Au surplus, en quel sens sont-ils compatibles avec la vie privée ? Ne devrait-on pas obtenir obligatoirement l’accord des individus pour les filmer et utiliser leurs données ? Doit-on accepter la banalisation de cette technologie et/ou bien ne l’autoriser qu’à titre tout à fait exceptionnel sinon l’interdire ? Dès lors qu’on l’autorise, comment protéger juridiquement les entreprises qui y ont recours ? Pareillement, quels moyens de recours donner aux « vidéosurveillés » ?
Pour l’heure, quelles dispositions juridiques nationales, européennes et internationale encadrent la vidéosurveillance intelligente ? Sont-elles efficaces et suffisantes pour garantir à la fois le respect des droits fondamentaux et de l’ordre public ? Il s’agira de répertorier autant les normes juridiques qui encadrent la vidéo-surveillance intelligente que leurs failles. Ce sont ces terrains qu’une fois identifiés permettront d’envisager les améliorations nécessaires.
Quelles recommandations juridiques pourrait-on formuler pour assurer que la vidéosurveillance intelligente s’inscrire davantage dans le cadre dans la philosophie de l’IA responsable ? Les citoyens ne devraient-ils pas avoir à s’exprimer systématiquement en amont de l’autorisation de la mise sur le marché de ce d’innovation technologique ? En un mot, quelles valeurs, quelles normes, quelles institutions, quelles procédures et quels mécanismes de contrôle doit-on inventer pour articuler la performance technologique-économique de la vidéosurveillance intelligente avec l’éthique des affaires et la démocratie ?
Surtout il s’agirait d’identifier les endroits précis de ces technologies où l’intervention du droit devrait être celle de la contrainte et ceux où, au contraire, l’encadrement des acteurs par l’engagement éthique pourrait être suffisant. Le projet peut ainsi servir de base pour se projeter sur la question plus globale de la limite de l’éthique comme moyen d’encadrement des nouvelles technologies. Non pas pour l’exclure mais pour lui donner sa juste place.
Cette partie « Éthique » du programme sera piloté par Matthieu Caron, maître de conférences en droit public à l’université Polytechnique des Hauts-de-France et directeur général de L’Observatoire de l’éthique publique. Elle donnera lieu à la production de deux ouvrages  ainsi qu’à la publication d’un livre blanc de propositions pour parfaire l’éthique du numérique en France.